{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM-umIlkElYB"
      },
      "source": [
        "# Benchmark lm-evaluation-harness over the model\n",
        "\n",
        "Authors: Giacomo Zuccarino, Jhon SebastiÃ¡n Moreno Triana.\n",
        "\n",
        "Programmed as part of the assignment of the course P2.11_advanced_DL_24_25.\n",
        "\n",
        "Professors: Alerto Cazzaniga, Cristiano de Nobili\n",
        "\n",
        "Program: Master in High Performance Computing.\n",
        "\n",
        "Institution: SISSA/ICTP, Trieste.\n",
        "\n",
        "---\n",
        "\n",
        "In the following notebook we use the lm-evaluation-harness for execute some benchmarks over the model after the Continued Pre Training (CPT) and fine tuning using LoRA adapters.\n",
        "\n",
        "> ğŸ“ <font color=\"DodgerBlue\"><b>NOTE</b></font>\n",
        ">\n",
        "> <font color=\"DodgerBlue\">If you want to check the CPT and fine tuning notebook you can go to [git hub notebook](./Gemma-3-4B-CPT-and-Fine-tuning.ipynb) if you want to open it from the github repository  or  [colab notebook](https://colab.research.google.com/drive/1Fn80nVlwy1vNqx8sMZ6ev4Um6KMNEkGf?usp=sharing) if you want to open it using the google colab platform.</front>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxWRYTN6bYDG"
      },
      "source": [
        "## 0. Initial setup and must run cells\n",
        "\n",
        "The following cells install all the packages needed for running the model's benchmark and setup variables for running the notebook with any unsloth gemma 3 model with LoRA adapters.\n",
        "\n",
        "> âš ï¸ <font color=\"GoldenRod\"><b>CAUTION</b> </font>\n",
        ">\n",
        "> <font color=\"GoldenRod\">Please read the instruction related to the unsloth model setup is very important if you want to run the unsloth model setup with a non default configuration. This setup instruction can be found on the [Readme file](github.com) at the github repository and/or in the [following subsection](#important-model-setup).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vlhajYgGMebJ",
        "outputId": "8e68ee3b-c290-4bbb-e691-54871ad09d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.4.7-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.4.7-py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unsloth\n",
            "Successfully installed unsloth-2025.4.7\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Collecting trl==0.15.2\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.4.4-py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading unsloth_zoo-2025.4.4-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 trl-0.15.2 unsloth_zoo-2025.4.4 xformers-0.0.29.post3\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, hf_xet, hf_transfer, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2025.4.4 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.4.4 requires tyro, which is not installed.\n",
            "unsloth 2025.4.7 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.4.4 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "unsloth 2025.4.7 requires protobuf<4.0.0, but you have protobuf 5.29.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 hf_transfer-0.1.9 hf_xet-1.1.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting sqlitedict\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sqlitedict\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=7192aa47f4f9461ed1e75eaefa31f7d1f527086c62399e762f3bf69272b60310\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "Successfully built sqlitedict\n",
            "Installing collected packages: sqlitedict, sacrebleu, portalocker, evaluate, colorama\n",
            "Successfully installed colorama-0.4.6 evaluate-0.4.3 portalocker-3.1.1 sacrebleu-2.5.1 sqlitedict-2.1.0\n",
            "Collecting lm-eval==0.4.8\n",
            "  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (1.6.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (3.5.1)\n",
            "Collecting jsonlines (from lm-eval==0.4.8)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (0.15.2)\n",
            "Collecting pybind11>=2.6.2 (from lm-eval==0.4.8)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm-eval==0.4.8)\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm-eval==0.4.8)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (1.6.1)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (2.6.0+cu124)\n",
            "Collecting tqdm-multiprocess (from lm-eval==0.4.8)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (4.51.3)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (0.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (0.3.8)\n",
            "Collecting word2number (from lm-eval==0.4.8)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm-eval==0.4.8) (10.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval==0.4.8) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval==0.4.8) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval==0.4.8) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval==0.4.8) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval==0.4.8) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval==0.4.8) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm-eval==0.4.8) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval==0.4.8) (3.11.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm-eval==0.4.8) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm-eval==0.4.8) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm-eval==0.4.8) (1.17.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.8) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.8) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.8) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.8) (5.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.4.8) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.4.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval==0.4.8) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->lm-eval==0.4.8)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval==0.4.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->lm-eval==0.4.8) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1->lm-eval==0.4.8) (0.21.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->lm-eval==0.4.8) (25.3.0)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm-eval==0.4.8) (75.2.0)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval==0.4.8)\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval==0.4.8)\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval==0.4.8)\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval==0.4.8)\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval==0.4.8)\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.8)\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.8) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.8) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.8) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.8) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.8) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->lm-eval==0.4.8) (1.20.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.8) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval==0.4.8) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval==0.4.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval==0.4.8) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm-eval==0.4.8) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.8) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.8) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->lm-eval==0.4.8) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score>=0.0.4->lm-eval==0.4.8) (8.1.8)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->lm-eval==0.4.8) (2025.2)\n",
            "Downloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Building wheels for collected packages: rouge-score, word2number\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=880f1801124d17d0ea149d0c018298c93ce9b21e77289f2807b7d9b95d20ea91\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=e0d0f01002f31a8d75cfbded6a090fcc3affae2cf92e722fa5cee78775d32750\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built rouge-score word2number\n",
            "Installing collected packages: word2number, tqdm-multiprocess, tcolorpy, pybind11, pathvalidate, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mbstrdecoder, jsonlines, typepy, rouge-score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, DataProperty, tabledata, pytablewriter, lm-eval\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed DataProperty-1.1.0 jsonlines-4.0.0 lm-eval-0.4.8 mbstrdecoder-1.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pathvalidate-3.2.3 pybind11-2.13.6 pytablewriter-1.2.1 rouge-score-0.1.2 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "# Installing unsloth without any dependency (those are overwrited by lm-eval)\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# Installing unsloth_zoo and some dependencies for using, saving, pushing and loading the models\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer hf_xet\n",
        "\n",
        "# Installing lm-eval varsion 0.4.8 and some pre-requisites\n",
        "# In this way lm-eval is going to install the lost dependencies of unsloth\n",
        "!pip install --no-deps sacrebleu portalocker colorama evaluate sqlitedict\n",
        "!pip install lm-eval==0.4.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobcsJ7PjxFL"
      },
      "source": [
        "<a name=\"important-model-setup\"></a>\n",
        "### IMPORTANT model setup\n",
        "\n",
        "> ğŸ“ <font color=\"DodgerBlue\"><b>NOTE</b> </font>\n",
        ">\n",
        "> <font color=\"DodgerBlue\">This part is needed just if you are using a different LoRA addapter and/or a different base model. In this case the variable `run_the_full_setup` in the following cells have to be turn to `True`</font>\n",
        "\n",
        "Running this code can be very callenging because of the unsloth+LoRA+gemma3 and lm-evaluation-harness uncompatibility. In this case we need to merge the base model with the LoRA adapter after the fine tuning, if this can sound an easy task it gets very difficult due to unsloth and gemma3 class wrapping that can be very divergent from a usual hugging face model.\n",
        "\n",
        "For make the run of the setup easier we setup some variables that can help to run the model better.\n",
        "\n",
        "---\n",
        "\n",
        "<h4><font color=\"red\">HOW TO RUN THE CODE (with enough gpu):</font></h4>\n",
        "\n",
        "If you have enough gpu you can just change the _hugginh face path_ related variables for the ones that you are using and set to `True` the `enough_gp` variable and run the code, that's all.\n",
        "\n",
        " ---\n",
        "\n",
        "<h4><font color=\"red\">HOW TO RUN THE CODE (without enough gpu):</font></h4>\n",
        "\n",
        "Without GPU we have to do more steps, but don't worry is not that hard. Now, `enough_gpu` variable should be set up to `False`, because of GPU V-RAM limitation you need to restart the colab session after you finnish the model saving local (that is why we are saving locally the models due to limitated GPU resources).\n",
        "\n",
        "In this case you need to run the [setup cell](#setup-cell) twice. The first time you need to run it with the variable `first_run` equals to `True`, then restart the sesion and set it to `False` and run it again. That's it.\n",
        "\n",
        "---\n",
        "\n",
        "**And finally...**\n",
        "\n",
        "You can run the `lm-eval` command with the local directory to your model or the _hugging face path_ to the final merged model (the same as the `merge_model_hf_path` variable) and the task taht you want to run.\n",
        "\n",
        "> ğŸ“ <font color=\"DodgerBlue\"><b>NOTE</b></font>\n",
        ">\n",
        "> <font color=\"DodgerBlue\"> We suggest you to push to hugging face if you want to run it in the future, so you don't have to restart everything from scratch and fou can go directly to the `lm-eval` command.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> âš¡ <font color=\"Tomato\"><b>IMPORTANT</b> </font>\n",
        ">\n",
        "> <font color=\"Tomato\" >Before you change the values of this variable please read the previous markdown cell.</font>\n"
      ],
      "metadata": {
        "id": "_q5NLkIJtl6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TDTu_IKgf1_F"
      },
      "outputs": [],
      "source": [
        "base_model_hf_path = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\"\n",
        "LoRA_adapter_hf_path = \"Jh0mpis/gemma-3b-physics-instruct-alpaca-v2\"\n",
        "merge_model_hf_path = \"Jh0mpis/gemma-3-4b-physics-instruct-alpaca-model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yg2acs__gcuU"
      },
      "outputs": [],
      "source": [
        "enough_gpu = False\n",
        "pushing_to_hf = False\n",
        "run_the_full_setup = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pC4orfJeqia"
      },
      "source": [
        "## Running the benchmark (+ unsloth model setup)\n",
        "\n",
        "Executing the `lm-eval` command alongside unsloth gemma 3 model is not a trivial task. For running the benchmaark we did the following:\n",
        "\n",
        "1. Install the dependencies that can work together.\n",
        "2. Merging the base model with the used LoRA adapter. Unsloth saves the model after the fine tuning as a LoRA adapter (this kind of model does not use files like `config.json` file), however, `lm-evaluation-harness` need a usual hugging face model (this is, a model with the proper configuration file). we follow the following steps:\n",
        "  1. Thats why we need to load the base model, in this case the `unsloth/gemma-3-4b-it-unsloth-bnb-4bit` model.\n",
        "  2. We need to save the model in a _hugging face format_ copying the tied weights manually to ensure consistency.\n",
        "  3. Save the base model locally.\n",
        "  4. Load the model as a `AutoModelForCausalLM` instance from the local presaved model.\n",
        "  5. Load the adapters using a `Peft` model.\n",
        "  6. Merge the model and the adapter.\n",
        "3. Save locally the final merged model that is compatible for `lm-evaluation-harness` and push it to hugging face (you can see the merged model at the [hugging face repository](https://huggingface.co/Jh0mpis/gemma-3-4b-physics-instruct-alpaca-model))\n",
        "4. Finally, we can run the `lm-eval` command using the hugging face merged model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btDwowZjp5Uu"
      },
      "source": [
        "<a name=\"setup-cell\"></a>\n",
        "### Setup cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0b2arIWBBFqv"
      },
      "outputs": [],
      "source": [
        "first_run = True # DO NOT FORGET TO CHANGE THIS IF enough_gpu IS FALSE AFTER YOU RUN THE FIRST TIME\n",
        "if run_the_full_setup:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    from peft import PeftModel\n",
        "\n",
        "    load_and_save_base_model = True\n",
        "    load_and_save_model_for_merge = True\n",
        "\n",
        "    if not enough_gpu:\n",
        "        if first_run:\n",
        "            load_and_save_model_for_merge = False\n",
        "        else:\n",
        "            load_and_save_base_model = False\n",
        "\n",
        "\n",
        "    # Step 1: Load original model, but untie embeddings\n",
        "    if load_and_save_base_model:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_hf_path,\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\",\n",
        "            tie_word_embeddings=False\n",
        "        )\n",
        "\n",
        "        # Step 2: Copy the tied weights manually to ensure consistency\n",
        "        model.language_model.lm_head.weight.data = model.language_model.model.embed_tokens.weight.data.clone()\n",
        "\n",
        "        # Step 3: Save this new base model (optional, for future reuse)\n",
        "        model.save_pretrained(\"untied_base_model\")\n",
        "        model.config.save_pretrained(\"untied_base_model\")\n",
        "\n",
        "    if load_and_save_model_for_merge:\n",
        "        # Load untied model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"untied_base_model\",\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Load adapter\n",
        "        peft_model = PeftModel.from_pretrained(base_model, LoRA_adapter_hf_path)\n",
        "\n",
        "        # Merge LoRA adapter\n",
        "        merged_model = peft_model.merge_and_unload()\n",
        "        # save the merged model\n",
        "        merged_model.save_pretrained(\"final_model\", safe_serialization=True)\n",
        "        # Re-load the tokenizer manually from the base model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(LoRA_adapter_hf_path)\n",
        "        # Save tokenizer files to the same directory as the merged model\n",
        "        tokenizer.save_pretrained(\"final_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oks8UfV6qI4z"
      },
      "source": [
        "### Pushing to hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TJEgOfNSBqhX"
      },
      "outputs": [],
      "source": [
        "if pushing_to_hf:\n",
        "    from huggingface_hub import HfApi\n",
        "\n",
        "    HfApi().upload_folder(\n",
        "        folder_path=\"final_model\",\n",
        "        repo_id=merge_model_hf_path,\n",
        "        commit_message=\"Merged base and LoRA adapter\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D37GDtHrXj2"
      },
      "source": [
        "### Creating a bash script file and running the `lm-eval` command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRzJDj1Fur9j"
      },
      "source": [
        "> ğŸ“ <font color=\"DodgerBlue\"><b>NOTE</b> </font>\n",
        ">\n",
        "> <font color=\"DodgerBlue\">If you have your own bash file you can upload it, ignore the next cell and run with it.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AbfVl9XdrmNT"
      },
      "outputs": [],
      "source": [
        "tasks_list = \"mmlu_stem,piqa,mmlu_pro_math,mmlu_pro_physics,mathqa,sciq\"\n",
        "\n",
        "config_file_content = f'''\n",
        "lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args \"pretrained={merge_model_hf_path},dtype=float16\" \\\n",
        "  --tasks {tasks_list} \\\n",
        "  --num_fewshot 5 \\\n",
        "  --limit 50 \\\n",
        "  --output_path \"./benchmark_results.json\" \\\n",
        "  --trust_remote_code\n",
        "'''\n",
        "\n",
        "with open(\"run_benchmark.sh\", \"w\") as file:\n",
        "    file.write(config_file_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF8muBim5Zgz",
        "outputId": "0766e27b-10b1-466d-bca1-bd1ed075dddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-07 10:21:28.933324: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-07 10:21:28.950813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746613288.972034    2129 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746613288.978430    2129 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-07 10:21:28.999908: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-07:10:21:45,299 WARNING  [lm_eval.__main__:316]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2025-05-07:10:21:45,307 INFO     [lm_eval.__main__:368] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
            "2025-05-07:10:21:45,308 INFO     [lm_eval.__main__:379] Selected Tasks: ['mathqa', 'mmlu_pro_math', 'mmlu_pro_physics', 'mmlu_stem', 'piqa', 'sciq']\n",
            "2025-05-07:10:21:45,311 INFO     [lm_eval.evaluator:169] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-05-07:10:21:45,311 INFO     [lm_eval.evaluator:206] Initializing hf model, with arguments: {'pretrained': 'Jh0mpis/gemma-3-4b-physics-instruct-alpaca-model', 'dtype': 'float16', 'trust_remote_code': True}\n",
            "2025-05-07:10:21:45,409 INFO     [lm_eval.models.huggingface:136] Using device 'cuda'\n",
            "config.json: 100% 5.79k/5.79k [00:00<00:00, 33.9MB/s]\n",
            "tokenizer_config.json: 100% 1.16M/1.16M [00:00<00:00, 16.1MB/s]\n",
            "tokenizer.model: 100% 4.69M/4.69M [00:00<00:00, 51.9MB/s]\n",
            "tokenizer.json: 100% 33.4M/33.4M [00:00<00:00, 143MB/s]\n",
            "added_tokens.json: 100% 35.0/35.0 [00:00<00:00, 295kB/s]\n",
            "special_tokens_map.json: 100% 670/670 [00:00<00:00, 7.17MB/s]\n",
            "2025-05-07:10:21:49,013 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "model.safetensors: 100% 4.56G/4.56G [00:11<00:00, 405MB/s]\n",
            "generation_config.json: 100% 210/210 [00:00<00:00, 1.85MB/s]\n",
            "2025-05-07:10:22:10,109 INFO     [lm_eval.models.huggingface:223] Model type is 'gemma3', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\n",
            "README.md: 100% 7.44k/7.44k [00:00<00:00, 44.6MB/s]\n",
            "math_qa.py: 100% 3.25k/3.25k [00:00<00:00, 34.1MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for math_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/math_qa\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading data: 100% 7.30M/7.30M [00:00<00:00, 80.0MB/s]\n",
            "Generating train split: 100% 29837/29837 [00:01<00:00, 22913.15 examples/s]\n",
            "Generating test split: 100% 2985/2985 [00:00<00:00, 23217.34 examples/s]\n",
            "Generating validation split: 100% 4475/4475 [00:00<00:00, 23380.49 examples/s]\n",
            "README.md: 100% 10.9k/10.9k [00:00<00:00, 69.6MB/s]\n",
            "test-00000-of-00001.parquet: 100% 4.16M/4.16M [00:00<00:00, 46.6MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 45.3k/45.3k [00:00<00:00, 177MB/s]\n",
            "Generating test split: 100% 12032/12032 [00:00<00:00, 198419.69 examples/s]\n",
            "Generating validation split: 100% 70/70 [00:00<00:00, 25943.38 examples/s]\n",
            "Filter: 100% 70/70 [00:00<00:00, 16044.66 examples/s]\n",
            "Filter: 100% 12032/12032 [00:00<00:00, 57166.53 examples/s]\n",
            "Filter: 100% 70/70 [00:00<00:00, 10880.57 examples/s]\n",
            "Filter: 100% 12032/12032 [00:00<00:00, 56134.06 examples/s]\n",
            "README.md: 100% 1.11k/1.11k [00:00<00:00, 9.51MB/s]\n",
            "mmlu_no_train.py: 100% 5.86k/5.86k [00:00<00:00, 45.5MB/s]\n",
            "data.tar: 100% 166M/166M [00:00<00:00, 298MB/s]\n",
            "Generating test split: 270 examples [00:00, 3330.41 examples/s]\n",
            "Generating validation split: 29 examples [00:00, 8191.45 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 59.97 examples/s]\n",
            "Generating test split: 135 examples [00:00, 1728.12 examples/s]\n",
            "Generating validation split: 14 examples [00:00, 5259.79 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.28 examples/s]\n",
            "Generating test split: 100 examples [00:00, 1349.51 examples/s]\n",
            "Generating validation split: 8 examples [00:00, 1908.24 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.55 examples/s]\n",
            "Generating test split: 100 examples [00:00, 1293.32 examples/s]\n",
            "Generating validation split: 11 examples [00:00, 3428.50 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.47 examples/s]\n",
            "Generating test split: 203 examples [00:00, 2561.68 examples/s]\n",
            "Generating validation split: 22 examples [00:00, 4778.35 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.74 examples/s]\n",
            "Generating test split: 235 examples [00:00, 2938.42 examples/s]\n",
            "Generating validation split: 26 examples [00:00, 8941.61 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.34 examples/s]\n",
            "Generating test split: 100 examples [00:00, 1334.40 examples/s]\n",
            "Generating validation split: 11 examples [00:00, 3168.34 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.11 examples/s]\n",
            "Generating test split: 100 examples [00:00, 1341.07 examples/s]\n",
            "Generating validation split: 9 examples [00:00, 3727.17 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.97 examples/s]\n",
            "Generating test split: 100 examples [00:00, 1296.72 examples/s]\n",
            "Generating validation split: 11 examples [00:00, 2833.29 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.10 examples/s]\n",
            "Generating test split: 100 examples [00:00, 1377.90 examples/s]\n",
            "Generating validation split: 11 examples [00:00, 3866.04 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.89 examples/s]\n",
            "Generating test split: 216 examples [00:00, 2622.40 examples/s]\n",
            "Generating validation split: 23 examples [00:00, 6125.02 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.14 examples/s]\n",
            "Generating test split: 152 examples [00:00, 2025.42 examples/s]\n",
            "Generating validation split: 16 examples [00:00, 6253.74 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 65.87 examples/s]\n",
            "Generating test split: 145 examples [00:00, 1963.23 examples/s]\n",
            "Generating validation split: 16 examples [00:00, 5007.75 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 65.59 examples/s]\n",
            "Generating test split: 310 examples [00:00, 3836.96 examples/s]\n",
            "Generating validation split: 32 examples [00:00, 8565.81 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.22 examples/s]\n",
            "Generating test split: 144 examples [00:00, 1944.65 examples/s]\n",
            "Generating validation split: 16 examples [00:00, 3630.45 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 67.31 examples/s]\n",
            "Generating test split: 102 examples [00:00, 1318.10 examples/s]\n",
            "Generating validation split: 11 examples [00:00, 3935.29 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.70 examples/s]\n",
            "Generating test split: 151 examples [00:00, 1931.04 examples/s]\n",
            "Generating validation split: 17 examples [00:00, 4760.53 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.59 examples/s]\n",
            "Generating test split: 378 examples [00:00, 4328.96 examples/s]\n",
            "Generating validation split: 41 examples [00:00, 8144.67 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.39 examples/s]\n",
            "Generating test split: 112 examples [00:00, 1516.87 examples/s]\n",
            "Generating validation split: 11 examples [00:00, 3990.43 examples/s]\n",
            "Generating dev split: 5 examples [00:00, 66.97 examples/s]\n",
            "README.md: 100% 8.41k/8.41k [00:00<00:00, 50.7MB/s]\n",
            "piqa.py: 100% 5.36k/5.36k [00:00<00:00, 46.6MB/s]\n",
            "Downloading data: 100% 1.82M/1.82M [00:00<00:00, 204MB/s]\n",
            "Downloading data: 100% 815k/815k [00:00<00:00, 14.9MB/s]\n",
            "Generating train split: 100% 16113/16113 [00:00<00:00, 32467.39 examples/s]\n",
            "Generating test split: 100% 3084/3084 [00:00<00:00, 33448.40 examples/s]\n",
            "Generating validation split: 100% 1838/1838 [00:00<00:00, 31885.03 examples/s]\n",
            "README.md: 100% 7.02k/7.02k [00:00<00:00, 47.6MB/s]\n",
            "train-00000-of-00001.parquet: 100% 3.99M/3.99M [00:00<00:00, 44.7MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 339k/339k [00:00<00:00, 431MB/s]\n",
            "test-00000-of-00001.parquet: 100% 343k/343k [00:00<00:00, 497MB/s]\n",
            "Generating train split: 100% 11679/11679 [00:00<00:00, 426857.18 examples/s]\n",
            "Generating validation split: 100% 1000/1000 [00:00<00:00, 244295.18 examples/s]\n",
            "Generating test split: 100% 1000/1000 [00:00<00:00, 248994.00 examples/s]\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of sciq from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of piqa from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_machine_learning from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_elementary_mathematics from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_high_school_physics from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_college_physics from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_college_biology from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_high_school_biology from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_electrical_engineering from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_astronomy from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_high_school_statistics from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_abstract_algebra from None to 5\n",
            "2025-05-07:10:22:32,706 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_college_computer_science from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_high_school_computer_science from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_computer_security from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_conceptual_physics from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_high_school_chemistry from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_college_mathematics from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_college_chemistry from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_anatomy from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_high_school_mathematics from None to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_pro_physics from 5 to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mmlu_pro_math from 5 to 5\n",
            "2025-05-07:10:22:32,707 WARNING  [lm_eval.evaluator:275] Overwriting default num_fewshot of mathqa from None to 5\n",
            "2025-05-07:10:22:32,709 INFO     [lm_eval.api.task:420] Building contexts for sciq on rank 0...\n",
            "100% 50/50 [00:00<00:00, 137.63it/s]\n",
            "2025-05-07:10:22:33,076 INFO     [lm_eval.api.task:420] Building contexts for piqa on rank 0...\n",
            "100% 50/50 [00:00<00:00, 188.00it/s]\n",
            "2025-05-07:10:22:33,344 INFO     [lm_eval.api.task:420] Building contexts for mmlu_machine_learning on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.60it/s]\n",
            "2025-05-07:10:22:33,702 INFO     [lm_eval.api.task:420] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.09it/s]\n",
            "2025-05-07:10:22:34,062 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_physics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.13it/s]\n",
            "2025-05-07:10:22:34,422 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_physics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.76it/s]\n",
            "2025-05-07:10:22:34,780 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_biology on rank 0...\n",
            "100% 50/50 [00:00<00:00, 139.25it/s]\n",
            "2025-05-07:10:22:35,142 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_biology on rank 0...\n",
            "100% 50/50 [00:00<00:00, 139.76it/s]\n",
            "2025-05-07:10:22:35,503 INFO     [lm_eval.api.task:420] Building contexts for mmlu_electrical_engineering on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.33it/s]\n",
            "2025-05-07:10:22:35,862 INFO     [lm_eval.api.task:420] Building contexts for mmlu_astronomy on rank 0...\n",
            "100% 50/50 [00:00<00:00, 139.59it/s]\n",
            "2025-05-07:10:22:36,224 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_statistics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 139.91it/s]\n",
            "2025-05-07:10:22:36,584 INFO     [lm_eval.api.task:420] Building contexts for mmlu_abstract_algebra on rank 0...\n",
            "100% 50/50 [00:00<00:00, 141.50it/s]\n",
            "2025-05-07:10:22:36,940 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_computer_science on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.58it/s]\n",
            "2025-05-07:10:22:37,299 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
            "100% 50/50 [00:00<00:00, 141.05it/s]\n",
            "2025-05-07:10:22:37,657 INFO     [lm_eval.api.task:420] Building contexts for mmlu_computer_security on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.26it/s]\n",
            "2025-05-07:10:22:38,016 INFO     [lm_eval.api.task:420] Building contexts for mmlu_conceptual_physics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.36it/s]\n",
            "2025-05-07:10:22:38,375 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
            "100% 50/50 [00:00<00:00, 141.65it/s]\n",
            "2025-05-07:10:22:38,731 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_mathematics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.93it/s]\n",
            "2025-05-07:10:22:39,088 INFO     [lm_eval.api.task:420] Building contexts for mmlu_college_chemistry on rank 0...\n",
            "100% 50/50 [00:00<00:00, 140.68it/s]\n",
            "2025-05-07:10:22:39,446 INFO     [lm_eval.api.task:420] Building contexts for mmlu_anatomy on rank 0...\n",
            "100% 50/50 [00:00<00:00, 141.67it/s]\n",
            "2025-05-07:10:22:39,802 INFO     [lm_eval.api.task:420] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 137.27it/s]\n",
            "2025-05-07:10:22:40,169 INFO     [lm_eval.api.task:420] Building contexts for mmlu_pro_physics on rank 0...\n",
            "100% 50/50 [00:00<00:00, 686.56it/s]\n",
            "2025-05-07:10:22:40,255 INFO     [lm_eval.api.task:420] Building contexts for mmlu_pro_math on rank 0...\n",
            "100% 50/50 [00:00<00:00, 675.45it/s]\n",
            "2025-05-07:10:22:40,339 INFO     [lm_eval.api.task:420] Building contexts for mathqa on rank 0...\n",
            "100% 50/50 [00:00<00:00, 155.93it/s]\n",
            "2025-05-07:10:22:40,663 INFO     [lm_eval.evaluator:517] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 4350/4350 [03:46<00:00, 19.24it/s]\n",
            "2025-05-07:10:26:34,715 INFO     [lm_eval.evaluator:517] Running generate_until requests\n",
            "Running generate_until requests:   0% 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "Running generate_until requests: 100% 100/100 [44:02<00:00, 26.43s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-05-07:11:10:40,412 INFO     [lm_eval.loggers.evaluation_tracker:209] Saving results aggregated\n",
            "hf (pretrained=Jh0mpis/gemma-3-4b-physics-instruct-alpaca-model,dtype=float16,trust_remote_code=True), gen_kwargs: (None), limit: 50.0, num_fewshot: 5, batch_size: 1\n",
            "|             Tasks             |Version|    Filter    |n-shot|  Metric   |   |Value |   |Stderr|\n",
            "|-------------------------------|------:|--------------|-----:|-----------|---|-----:|---|-----:|\n",
            "|mathqa                         |      1|none          |     5|acc        |â†‘  |0.2200|Â±  |0.0592|\n",
            "|                               |       |none          |     5|acc_norm   |â†‘  |0.2200|Â±  |0.0592|\n",
            "|math                           |      1|custom-extract|     5|exact_match|â†‘  |0.0000|Â±  |0.0000|\n",
            "|physics                        |      1|custom-extract|     5|exact_match|â†‘  |0.0000|Â±  |0.0000|\n",
            "|stem                           |      2|none          |      |acc        |â†‘  |0.2263|Â±  |0.0136|\n",
            "| - abstract_algebra            |      1|none          |     5|acc        |â†‘  |0.2000|Â±  |0.0571|\n",
            "| - anatomy                     |      1|none          |     5|acc        |â†‘  |0.1600|Â±  |0.0524|\n",
            "| - astronomy                   |      1|none          |     5|acc        |â†‘  |0.1400|Â±  |0.0496|\n",
            "| - college_biology             |      1|none          |     5|acc        |â†‘  |0.2800|Â±  |0.0641|\n",
            "| - college_chemistry           |      1|none          |     5|acc        |â†‘  |0.2000|Â±  |0.0571|\n",
            "| - college_computer_science    |      1|none          |     5|acc        |â†‘  |0.2800|Â±  |0.0641|\n",
            "| - college_mathematics         |      1|none          |     5|acc        |â†‘  |0.2400|Â±  |0.0610|\n",
            "| - college_physics             |      1|none          |     5|acc        |â†‘  |0.1600|Â±  |0.0524|\n",
            "| - computer_security           |      1|none          |     5|acc        |â†‘  |0.2600|Â±  |0.0627|\n",
            "| - conceptual_physics          |      1|none          |     5|acc        |â†‘  |0.3200|Â±  |0.0666|\n",
            "| - electrical_engineering      |      1|none          |     5|acc        |â†‘  |0.3200|Â±  |0.0666|\n",
            "| - elementary_mathematics      |      1|none          |     5|acc        |â†‘  |0.2200|Â±  |0.0592|\n",
            "| - high_school_biology         |      1|none          |     5|acc        |â†‘  |0.1600|Â±  |0.0524|\n",
            "| - high_school_chemistry       |      1|none          |     5|acc        |â†‘  |0.2000|Â±  |0.0571|\n",
            "| - high_school_computer_science|      1|none          |     5|acc        |â†‘  |0.2600|Â±  |0.0627|\n",
            "| - high_school_mathematics     |      1|none          |     5|acc        |â†‘  |0.2200|Â±  |0.0592|\n",
            "| - high_school_physics         |      1|none          |     5|acc        |â†‘  |0.1800|Â±  |0.0549|\n",
            "| - high_school_statistics      |      1|none          |     5|acc        |â†‘  |0.2000|Â±  |0.0571|\n",
            "| - machine_learning            |      1|none          |     5|acc        |â†‘  |0.3000|Â±  |0.0655|\n",
            "|piqa                           |      1|none          |     5|acc        |â†‘  |0.4600|Â±  |0.0712|\n",
            "|                               |       |none          |     5|acc_norm   |â†‘  |0.4600|Â±  |0.0712|\n",
            "|sciq                           |      1|none          |     5|acc        |â†‘  |0.0000|Â±  |0.0000|\n",
            "|                               |       |none          |     5|acc_norm   |â†‘  |0.0000|Â±  |0.0000|\n",
            "\n",
            "|Groups|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
            "|------|------:|------|------|------|---|-----:|---|-----:|\n",
            "|stem  |      2|none  |      |acc   |â†‘  |0.2263|Â±  |0.0136|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash run_benchmark.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ahjoJRiXx2Z3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "btDwowZjp5Uu"
      ],
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}